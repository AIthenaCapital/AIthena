{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the package working or not\n",
    "###  - test tensorflow and pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Normalize RSI  Stedev from 20 MA  MACD n.f  dist from 50MA  \\\n",
      "73 2012-04-15       0.651163           1.168733  0.921887       -0.014044   \n",
      "74 2012-04-16      -0.210526           0.573481  0.973192       -0.006004   \n",
      "75 2012-04-17       0.151899           1.127236  1.062393       -0.015904   \n",
      "76 2012-04-18       0.943820           2.406680  1.283761       -0.041836   \n",
      "77 2012-04-19       0.977778           2.155484  1.479964       -0.044475   \n",
      "78 2012-04-20       1.481481           2.932215  1.851736       -0.080374   \n",
      "79 2012-04-21       2.505495           1.928127  2.021115       -0.062510   \n",
      "80 2012-04-22       1.885057           1.375297  2.081530       -0.049423   \n",
      "81 2012-04-23       0.349515          -0.135091  1.810204       -0.002863   \n",
      "82 2012-04-24       0.912281           0.679868  1.742938       -0.029765   \n",
      "83 2012-04-25       0.740741           0.791767  1.693208       -0.034893   \n",
      "84 2012-04-26       0.654545           0.557417  1.583709       -0.028588   \n",
      "85 2012-04-27       0.623853           0.547273  1.486807       -0.029785   \n",
      "86 2012-04-28       0.066667          -0.438957  1.212702       -0.003976   \n",
      "87 2012-04-29      -0.220472          -1.054380  0.873982        0.012571   \n",
      "88 2012-04-30       0.062500          -0.759095  0.672018        0.002505   \n",
      "\n",
      "          RSI  Price  \n",
      "73  58.139535   4.97  \n",
      "74  47.368421   4.93  \n",
      "75  51.898734   4.98  \n",
      "76  61.797753   5.12  \n",
      "77  62.222222   5.14  \n",
      "78  68.518519   5.35  \n",
      "79  81.318681   5.26  \n",
      "80  73.563218   5.20  \n",
      "81  54.368932   4.96  \n",
      "82  61.403509   5.10  \n",
      "83  59.259259   5.13  \n",
      "84  58.181818   5.10  \n",
      "85  57.798165   5.11  \n",
      "86  50.833333   4.98  \n",
      "87  47.244094   4.90  \n",
      "88  50.781250   4.95  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('BTC daily_database.xlsx', sheet_name=1)\n",
    "# testing the input is correct or not\n",
    "print(df[73:89])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(0,dtype=tf.float32)\n",
    "cost = tf.add(tf.add(w**2,tf.multiply(-10.,w)),25)\n",
    "train = tf.train.GradientDescentOptimizer(0.01).minimize(cost)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "print(sess.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.099999994\n"
     ]
    }
   ],
   "source": [
    "sess.run(train)\n",
    "print(sess.run(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9999886\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    sess.run(train)\n",
    "print(sess.run(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (4, 2208)\n",
      "number of training samples = 2208\n",
      "number of variables = 4\n",
      "                        75         76         77         78         79    \\\n",
      "Stedev from 20 MA   1.127236   2.406680   2.155484   2.932215   1.928127   \n",
      "MACD n.f            1.062393   1.283761   1.479964   1.851736   2.021115   \n",
      "dist from 50MA     -0.015904  -0.041836  -0.044475  -0.080374  -0.062510   \n",
      "RSI                51.898734  61.797753  62.222222  68.518519  81.318681   \n",
      "\n",
      "                        80         81         82         83         84    \\\n",
      "Stedev from 20 MA   1.375297  -0.135091   0.679868   0.791767   0.557417   \n",
      "MACD n.f            2.081530   1.810204   1.742938   1.693208   1.583709   \n",
      "dist from 50MA     -0.049423  -0.002863  -0.029765  -0.034893  -0.028588   \n",
      "RSI                73.563218  54.368932  61.403509  59.259259  58.181818   \n",
      "\n",
      "                     ...           2273       2274       2275       2276  \\\n",
      "Stedev from 20 MA    ...      -0.799475  -2.254130  -1.936735  -1.309428   \n",
      "MACD n.f             ...       0.555496   0.330471   0.162090   0.070061   \n",
      "dist from 50MA       ...      -0.081363  -0.017043  -0.023607  -0.047618   \n",
      "RSI                  ...      46.028774  42.858803  35.728060  38.867127   \n",
      "\n",
      "                        2277       2278       2279       2280       2281  \\\n",
      "Stedev from 20 MA  -1.213211  -1.563939  -1.602563  -1.916435  -1.391756   \n",
      "MACD n.f           -0.005094  -0.100622  -0.195050  -0.320707  -0.379318   \n",
      "dist from 50MA     -0.045421  -0.021903  -0.006808   0.029849   0.009622   \n",
      "RSI                40.688668  40.296535  35.444117  19.835033  25.209970   \n",
      "\n",
      "                        2282  \n",
      "Stedev from 20 MA  -1.259672  \n",
      "MACD n.f           -0.423121  \n",
      "dist from 50MA      0.013988  \n",
      "RSI                20.958596  \n",
      "\n",
      "[4 rows x 2208 columns]\n"
     ]
    }
   ],
   "source": [
    "inputX = df.iloc[75:,2:6 ]\n",
    "inputX = inputX.T\n",
    "inputY = None\n",
    "print(\"X_train shape: \" + str(inputX.shape))\n",
    "print(\"number of training samples = \"+ str(inputX.shape[1]))\n",
    "print(\"number of variables = \" + str(inputX.shape[0]))\n",
    "print(inputX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## layer size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_x's shape = 4\n",
      "first layer(n_h)'s shape = 5\n"
     ]
    }
   ],
   "source": [
    "n_x = inputX.shape[0] # size of input layer\n",
    "n_h = 5\n",
    "n_y = 5\n",
    "print(\"n_x's shape = \" + str(n_x))\n",
    "print(\"first layer(n_h)'s shape = \" + str(n_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create placeholder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \n",
    "    #Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape = [n_x,None])\n",
    "    Y = tf.placeholder(tf.float32, shape = [n_y, None])  \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = Tensor(\"Placeholder:0\", shape=(4, ?), dtype=float32)\n",
      "Y = Tensor(\"Placeholder_1:0\", shape=(5, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X, Y = create_placeholders(n_x,n_y)\n",
    "print (\"X = \" + str(X))\n",
    "print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_parameters(n_x, n_h):\n",
    "    #Initializes weight parameters to build a neural network with tensorflow  \n",
    "    #tf.set_random_seed()                              \n",
    "    W1 = tf.get_variable(\"W1\", [n_h,n_x], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    b1 = tf.get_variable(\"b1\", [n_h,1], initializer = tf.zeros_initializer())\n",
    "    W2 = tf.get_variable(\"W2\", [5, n_h], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    b2 = tf.get_variable(\"b2\", [5,1], initializer = tf.zeros_initializer())\n",
    "    W3 = tf.get_variable(\"W3\", [5, 5], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    b3 = tf.get_variable(\"b3\", [5,1], initializer = tf.zeros_initializer())\n",
    "    W4 = tf.get_variable(\"W4\", [5, 5], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    b4 = tf.get_variable(\"b4\", [5,1], initializer = tf.zeros_initializer())\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3,\n",
    "                  \"W4\": W4,\n",
    "                  \"b4\": b4}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = <tf.Variable 'W1:0' shape=(5, 4) dtype=float32_ref>\n",
      "b1 = <tf.Variable 'b1:0' shape=(5, 1) dtype=float32_ref>\n",
      "W2 = <tf.Variable 'W2:0' shape=(5, 5) dtype=float32_ref>\n",
      "b2 = <tf.Variable 'b2:0' shape=(5, 1) dtype=float32_ref>\n",
      "W3 = <tf.Variable 'W3:0' shape=(5, 5) dtype=float32_ref>\n",
      "b3 = <tf.Variable 'b3:0' shape=(5, 1) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    parameters = initialize_parameters(n_x, n_h)\n",
    "    print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "    print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "    print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "    print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "    print(\"W3 = \" + str(parameters[\"W3\"]))\n",
    "    print(\"b3 = \" + str(parameters[\"b3\"]))\n",
    "    print(\"W4 = \" + str(parameters[\"W4\"]))\n",
    "    print(\"b4 = \" + str(parameters[\"b4\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_matrix(labels, C):\n",
    "    \"\"\"\n",
    "    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n",
    "                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n",
    "                     will be 1. \n",
    "                     \n",
    "    Arguments:\n",
    "    labels -- vector containing the labels \n",
    "    C -- number of classes, the depth of the one hot dimension\n",
    "    \n",
    "    Returns: \n",
    "    one_hot -- one hot matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Create a tf.constant equal to C (depth), name it 'C'. (approx. 1 line)\n",
    "    C = tf.constant(C, name = \"C\")\n",
    "    \n",
    "    # Use tf.one_hot, be careful with the axis (approx. 1 line)\n",
    "    one_hot_matrix = tf.one_hot(indices = labels, depth = C,axis  =0)\n",
    "    \n",
    "    # Create the session (approx. 1 line)\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    # Run the session (approx. 1 line)\n",
    "    one_hot = sess.run(one_hot_matrix)\n",
    "    \n",
    "    # Close the session (approx. 1 line). See method 1 above.\n",
    "    sess.close()\n",
    "\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot = [[0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([1,2,3,0,2,1])\n",
    "one_hot = one_hot_matrix(labels, C = 4)\n",
    "print (\"one_hot = \" + str(one_hot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Y-hat to softmax matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y = \n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "inputY \n",
    "Y = one_hot_matrix(inputY, C = 5)\n",
    "print(\"Y = \")\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> Sigmoid -> LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\", \"W4\", \"b4\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z4 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    W4 = parameters['W4']\n",
    "    b4 = parameters['b4']\n",
    "    \n",
    "                                                                     # Numpy Equivalents:\n",
    "    Z1 = tf.add(tf.matmul(W1,X) , b1 )                               # Z1 = np.dot(W1, X) + b1\n",
    "    A1 = tf.nn.sigmoid(Z1)                                           # A1 = sigmoid(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2,A1), b2 )                               # Z2 = np.dot(W2, a1) + b2\n",
    "    A2 = tf.nn.relu(Z2)                                              # A2 = relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(W3,A2), b3)                                # Z3 = np.dot(W3,Z2) + b3\n",
    "    A3 = tf.nn.relu(Z3)                                              # A3 = relu(Z2)\n",
    "    Z4 = tf.add(tf.matmul(W4,A3), b4)                                # Z4 = np.dot(W3,Z2) + b3 \n",
    "    \n",
    "    return Z4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z4, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z4 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z4\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z4)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    #compute cost\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_placeholders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-32d95c2ddfc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_placeholders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12288\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitialize_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mZ4\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_placeholders' is not defined"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(12288, 6)\n",
    "    parameters = initialize_parameters()\n",
    "    Z4 = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(Z4, Y)\n",
    "    print(\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(inputX, inputY, learning_rate = 0.0001,\n",
    "          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = inputX.shape                           # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = inputY.shape[0]                             # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters()\n",
    "\n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    Z4 = forward_propagation(X, parameters)\n",
    "\n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(Z4, Y)\n",
    "\n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "\n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "\n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "\n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z4), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the model \n",
    "parameters = model(X_train, Y_train, X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
