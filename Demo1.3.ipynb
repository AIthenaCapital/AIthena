{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date  Price       RSI  Stedev from 20 MA  MACD n.f  50MA_N.F.  \\\n",
      "0 2011-12-16   3.25  0.514151           0.591527  1.594689   0.500194   \n",
      "1 2011-12-18   3.25  0.443243           0.516482  1.489778   0.571086   \n",
      "2 2011-12-19   3.50  0.511962           1.311777  1.473728   1.266368   \n",
      "3 2011-12-20   4.75  0.822695           3.461711  1.840524   3.831441   \n",
      "4 2011-12-21   4.38  0.753247           2.180044  1.969366   2.760808   \n",
      "\n",
      "   20D PMO  35D PMO        ADX  actions  y-hat  Unnamed: 11  Unnamed: 12  \n",
      "0    3.270     0.59  69.772809      NaN      2            1  2157.000000  \n",
      "1    3.690     0.46  70.038037      NaN      2            1     0.912437  \n",
      "2    4.275     0.67  70.475308      NaN      2            1          NaN  \n",
      "3    5.140     1.55  68.503186      NaN      2            1          NaN  \n",
      "4    5.910     1.28  66.671929      NaN      2            1          NaN  \n"
     ]
    }
   ],
   "source": [
    "dfz = pd.read_excel('BTC daily_database.xlsx', sheet_name=0)\n",
    "# testing the input is correct or not\n",
    "print(dfz.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomize sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffler(filename):\n",
    "  dfs = pd.read_excel(filename, sheet_name=0, header=0)\n",
    "  # return the pandas dataframe\n",
    "  return dfs.reindex(np.random.permutation(dfs.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date    Price       RSI  Stedev from 20 MA  MACD n.f  50MA_N.F.  \\\n",
      "1097 2014-12-18   321.96  0.216281          -2.309140 -0.779016  -1.769375   \n",
      "1320 2015-08-01   284.69  0.570808          -0.126774  0.058674   0.897434   \n",
      "400  2013-01-20    15.48  0.869159           2.003732  2.068432   3.162742   \n",
      "2139 2017-10-28  5773.03  0.531117           0.517208  0.586384   1.365193   \n",
      "1389 2015-10-09   243.29  0.657833           1.347194  0.836023   1.557286   \n",
      "\n",
      "      20D PMO  35D PMO        ADX  actions  y-hat  Unnamed: 11  Unnamed: 12  \n",
      "1097    1.590    -0.78  17.291235      NaN      2            1          NaN  \n",
      "1320    5.600     0.49  23.521543      NaN      2            1          NaN  \n",
      "400     2.080     0.43  13.695088      NaN      2            1          NaN  \n",
      "2139   10.315     1.41  24.472306      NaN      2            1          NaN  \n",
      "1389    0.745     0.21  12.479787      NaN      2            1          NaN  \n"
     ]
    }
   ],
   "source": [
    "df = shuffler('BTC daily_database.xlsx')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputX shape: (7, 2363)\n",
      "number of training samples = 2363\n",
      "number of variables = 7\n",
      "[[ 0.21628076  0.57080777  0.86915888 ...  0.40165755  0.4970036\n",
      "   0.5437831 ]\n",
      " [-2.30913967 -0.1267741   2.00373151 ... -0.43280589 -0.49062454\n",
      "   0.49431104]\n",
      " [-0.77901612  0.05867444  2.06843183 ... -1.18617271 -0.75054959\n",
      "  -0.25738054]\n",
      " ...\n",
      " [ 1.59        5.6         2.08       ...  0.45       -4.915\n",
      "   7.45      ]\n",
      " [-0.78        0.49        0.43       ... -0.18       -0.47\n",
      "   0.61      ]\n",
      " [17.29123458 23.52154337 13.69508822 ... 13.589044   25.05257958\n",
      "  12.96145259]]\n",
      "(1, 2363)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\john liu\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\john liu\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "inputX = df.iloc[:,2:9 ].as_matrix()\n",
    "inputX = inputX.T\n",
    "inputY = df.iloc[:, 10:11].as_matrix()\n",
    "inputY = inputY.T\n",
    "print(\"inputX shape: \" + str(inputX.shape))\n",
    "print(\"number of training samples = \"+ str(inputX.shape[1]))\n",
    "print(\"number of variables = \" + str(inputX.shape[0]))\n",
    "print(inputX)\n",
    "print(inputY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 2, ..., 2, 2, 2]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperating to train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX's shape(7, 1893)\n",
      "testX's shape(7, 470)\n",
      "trainX's shape(1, 1893)\n",
      "testY's shape(1, 470)\n"
     ]
    }
   ],
   "source": [
    "trainX = inputX[:, : -470]\n",
    "trainY = inputY[:, : -470]\n",
    "testX = inputX[:, -470: ]\n",
    "testY = inputY[:, -470: ]\n",
    "print(\"trainX's shape\" + str(trainX.shape))\n",
    "print(\"testX's shape\" + str(testX.shape))\n",
    "print(\"trainX's shape\" + str(trainY.shape))\n",
    "print(\"testY's shape\" + str(testY.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer size inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_x's shape = 7\n",
      "first layer(n_h)'s shape = 10\n",
      "n_y's output = 1\n"
     ]
    }
   ],
   "source": [
    "n_x = inputX.shape[0] # size of input layer\n",
    "n_h = 10\n",
    "n_y = inputY.shape[0]\n",
    "print(\"n_x's shape = \" + str(n_x))\n",
    "print(\"first layer(n_h)'s shape = \" + str(n_h))\n",
    "print(\"n_y's output = \" + str(n_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create placeholder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \n",
    "    #Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape = [n_x,None])\n",
    "    Y = tf.placeholder(tf.float32, shape = [n_y, 1, None])  \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = Tensor(\"Placeholder:0\", shape=(7, ?), dtype=float32)\n",
      "(7, ?)\n",
      "Y = Tensor(\"Placeholder_1:0\", shape=(1, 1, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X, Y = create_placeholders(n_x,n_y)\n",
    "print (\"X = \" + str(X))\n",
    "print(X.shape)\n",
    "print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_parameters(n_x, n_h):\n",
    "    #Initializes weight parameters to build a neural network with tensorflow  \n",
    "    #tf.set_random_seed()                              \n",
    "    W1 = tf.get_variable(\"W1\", [n_h,n_x], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    b1 = tf.get_variable(\"b1\", [n_h,1], initializer = tf.zeros_initializer())\n",
    "    W2 = tf.get_variable(\"W2\", [10, n_h], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    b2 = tf.get_variable(\"b2\", [10, 1], initializer = tf.zeros_initializer())\n",
    "    W3 = tf.get_variable(\"W3\", [25, 10], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    b3 = tf.get_variable(\"b3\", [25, 1], initializer = tf.zeros_initializer())\n",
    "    W4 = tf.get_variable(\"W4\", [10, 25], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    b4 = tf.get_variable(\"b4\", [10,1], initializer = tf.zeros_initializer())\n",
    "    W5 = tf.get_variable(\"W5\", [5, 10], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    b5 = tf.get_variable(\"b5\", [5,1], initializer = tf.zeros_initializer())\n",
    "    #W6 = tf.get_variable(\"W6\", [5, 10], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    #b6 = tf.get_variable(\"b6\", [5,1], initializer = tf.zeros_initializer())\n",
    "    #W7 = tf.get_variable(\"W7\", [5, 10], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    #b7 = tf.get_variable(\"b7\", [5,1], initializer = tf.zeros_initializer())\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3,\n",
    "                  \"W4\": W4,\n",
    "                  \"b4\": b4,\n",
    "                  \"W5\": W5,\n",
    "                  \"b5\": b5}\n",
    "                  #\"W6\": W6,\n",
    "                  #\"b6\": b6,\n",
    "                  #\"W7\": W7,\n",
    "                  #\"b7\": b7}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = <tf.Variable 'W1:0' shape=(10, 7) dtype=float32_ref>\n",
      "b1 = <tf.Variable 'b1:0' shape=(10, 1) dtype=float32_ref>\n",
      "W2 = <tf.Variable 'W2:0' shape=(10, 10) dtype=float32_ref>\n",
      "b2 = <tf.Variable 'b2:0' shape=(10, 1) dtype=float32_ref>\n",
      "W3 = <tf.Variable 'W3:0' shape=(25, 10) dtype=float32_ref>\n",
      "b3 = <tf.Variable 'b3:0' shape=(25, 1) dtype=float32_ref>\n",
      "W4 = <tf.Variable 'W4:0' shape=(10, 25) dtype=float32_ref>\n",
      "b4 = <tf.Variable 'b4:0' shape=(10, 1) dtype=float32_ref>\n",
      "W5 = <tf.Variable 'W5:0' shape=(5, 10) dtype=float32_ref>\n",
      "b5 = <tf.Variable 'b5:0' shape=(5, 1) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    parameters = initialize_parameters(n_x, n_h)\n",
    "    print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "    print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "    print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "    print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "    print(\"W3 = \" + str(parameters[\"W3\"]))\n",
    "    print(\"b3 = \" + str(parameters[\"b3\"]))\n",
    "    print(\"W4 = \" + str(parameters[\"W4\"]))\n",
    "    print(\"b4 = \" + str(parameters[\"b4\"]))\n",
    "    print(\"W5 = \" + str(parameters[\"W5\"]))\n",
    "    print(\"b5 = \" + str(parameters[\"b5\"]))\n",
    "    #print(\"W6 = \" + str(parameters[\"W6\"]))\n",
    "    #print(\"b6 = \" + str(parameters[\"b6\"]))\n",
    "    #print(\"W7 = \" + str(parameters[\"W7\"]))\n",
    "    #print(\"b7 = \" + str(parameters[\"b7\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_matrix(labels, C):\n",
    "    \"\"\"\n",
    "    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n",
    "                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n",
    "                     will be 1. \n",
    "                     \n",
    "    Arguments:\n",
    "    labels -- vector containing the labels \n",
    "    C -- number of classes, the depth of the one hot dimension\n",
    "    \n",
    "    Returns: \n",
    "    one_hot -- one hot matrix\n",
    "    \"\"\"\n",
    "    # Create a tf.constant equal to C (depth), name it 'C'. (approx. 1 line)\n",
    "    C = tf.constant(C, name = \"C\")\n",
    "    \n",
    "    # Use tf.one_hot, be careful with the axis (approx. 1 line)\n",
    "    one_hot_matrix = tf.one_hot(indices = labels, depth = C,axis  =0)\n",
    "    \n",
    "    # Create the session (approx. 1 line)\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    # Run the session (approx. 1 line)\n",
    "    one_hot = sess.run(one_hot_matrix)\n",
    "    \n",
    "    # Close the session (approx. 1 line). See method 1 above.\n",
    "    sess.close()\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot = [[0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([1,2,3,0,2,1])\n",
    "one_hot = one_hot_matrix(labels, C = 4)\n",
    "print (\"one_hot = \" + str(one_hot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Y-hat to softmax matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y = \n",
      "[[[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. ... 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]]]\n",
      "trainY-shape = (5, 1, 1893)\n",
      "testY-shape = (5, 1, 470)\n"
     ]
    }
   ],
   "source": [
    "trainY = one_hot_matrix(trainY, C = 5)\n",
    "testY = one_hot_matrix(testY, C = 5) \n",
    "print(\"Y = \")\n",
    "print(trainY)\n",
    "print(\"trainY-shape = \" + str(trainY.shape))\n",
    "print(\"testY-shape = \" + str(testY.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\", \"W4\", \"b4\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z4 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    W4 = parameters['W4']\n",
    "    b4 = parameters['b4']\n",
    "    W5 = parameters['W5']\n",
    "    b5 = parameters['b5'] \n",
    "    #W6 = parameters['W6']\n",
    "    #b6 = parameters['b6'] \n",
    "    #W7 = parameters['W7']\n",
    "    #b7 = parameters['b7'] \n",
    "                                                                     \n",
    "    Z1 = tf.add(tf.matmul(W1,X) , b1 )                              \n",
    "    A1 = tf.nn.tanh(Z1)                                                  \n",
    "    Z2 = tf.add(tf.matmul(W2,A1), b2 )                               \n",
    "    A2 = tf.nn.sigmoid(Z2)                                                  \n",
    "    Z3 = tf.add(tf.matmul(W3,A2), b3)\n",
    "    A3 = tf.nn.selu(Z3)                                         \n",
    "    Z4 = tf.add(tf.matmul(W4,A3), b4)                                \n",
    "    A4 = tf.nn.selu(Z4)                                                  \n",
    "    Z5 = tf.add(tf.matmul(W5,A4), b5)                                 \n",
    "    #A5 = tf.nn.tanh(Z5)                                              \n",
    "    #Z6 = tf.add(tf.matmul(W6,A5), b6)                                 \n",
    "    #A6 = tf.nn.relu(Z6)                                              \n",
    "    #Z7 = tf.add(tf.matmul(W7,A6), b7)                                \n",
    "    return Z5\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z4 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z4\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    #compute cost\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-237d1ee019a2>:18: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "cost = Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(4, 1)\n",
    "    parameters = initialize_parameters(4,1)\n",
    "    Z1 = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(Z1, Y)\n",
    "    print(\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "\n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    #permutation = list(np.random.permutation(m))\n",
    "    #shuffled_X = X[:, permutation]\n",
    "    #shuffled_Y = Y[:, 1, permutation]\n",
    "    \n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = X[:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "        mini_batch_Y = Y[:,:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = X[:, num_complete_minibatches*mini_batch_size : m]\n",
    "        mini_batch_Y = Y[:,:, num_complete_minibatches*mini_batch_size : m]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the 1st mini_batch_X: (7, 64)\n",
      "shape of the 2nd mini_batch_X: (7, 64)\n",
      "shape of the 3rd mini_batch_X: (7, 64)\n",
      "shape of the last mini_batch_X: (7, 37)\n",
      "shape of the 1st mini_batch_Y: (5, 1, 64)\n",
      "shape of the 2nd mini_batch_Y: (5, 1, 64)\n",
      "shape of the 3rd mini_batch_Y: (5, 1, 64)\n",
      "shape of the last mini_batch_Y: (5, 1, 37)\n"
     ]
    }
   ],
   "source": [
    "mini_batches = random_mini_batches(trainX, trainY, 64, seed=0)\n",
    "\n",
    "print (\"shape of the 1st mini_batch_X: \" + str(mini_batches[0][0].shape))\n",
    "print (\"shape of the 2nd mini_batch_X: \" + str(mini_batches[1][0].shape))\n",
    "print (\"shape of the 3rd mini_batch_X: \" + str(mini_batches[2][0].shape))\n",
    "print (\"shape of the last mini_batch_X: \" + str(mini_batches[-1][0].shape))\n",
    "print (\"shape of the 1st mini_batch_Y: \" + str(mini_batches[0][1].shape))\n",
    "print (\"shape of the 2nd mini_batch_Y: \" + str(mini_batches[1][1].shape)) \n",
    "print (\"shape of the 3rd mini_batch_Y: \" + str(mini_batches[2][1].shape))\n",
    "print (\"shape of the last mini_batch_Y: \" + str(mini_batches[-1][1].shape))\n",
    "#print (\"mini batch sanity check: \")\n",
    "#mini_batches[0][1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(trainX, trainY, testX, testY, learning_rate = 0.0013,\n",
    "          num_epochs = 2200, minibatch_size = 64, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = trainX.shape                                # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = trainY.shape[0]                                  # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_x, n_h)\n",
    "\n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    Z = forward_propagation(X, parameters)\n",
    "    \n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(Z,Y)\n",
    "\n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "\n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(trainX, trainY, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "\n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "\n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: trainX, Y: trainY}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: testX, Y: testY}))\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.635227\n",
      "Cost after epoch 100: 0.286331\n",
      "Cost after epoch 200: 0.271589\n",
      "Cost after epoch 300: 0.261557\n",
      "Cost after epoch 400: 0.253338\n",
      "Cost after epoch 500: 0.248294\n",
      "Cost after epoch 600: 0.243008\n",
      "Cost after epoch 700: 0.236679\n",
      "Cost after epoch 800: 0.230450\n",
      "Cost after epoch 900: 0.224101\n",
      "Cost after epoch 1000: 0.218030\n",
      "Cost after epoch 1100: 0.216574\n",
      "Cost after epoch 1200: 0.207144\n",
      "Cost after epoch 1300: 0.203363\n",
      "Cost after epoch 1400: 0.200223\n",
      "Cost after epoch 1500: 0.196781\n",
      "Cost after epoch 1600: 0.191749\n",
      "Cost after epoch 1700: 0.188497\n",
      "Cost after epoch 1800: 0.185459\n",
      "Cost after epoch 1900: 0.182917\n",
      "Cost after epoch 2000: 0.180850\n",
      "Cost after epoch 2100: 0.177701\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n",
      "Train Accuracy: 0.9445325\n",
      "Test Accuracy: 0.8808511\n"
     ]
    }
   ],
   "source": [
    "# run the model \n",
    "parameters = model(trainX, trainY, testX,testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
