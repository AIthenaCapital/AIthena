{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date  Price       RSI  Stedev from 20 MA  MACD n.f  50MA_N.F.  \\\n",
      "0 2011-12-16   3.25  0.514151           0.591527  1.594689   0.500194   \n",
      "1 2011-12-18   3.25  0.443243           0.516482  1.489778   0.571086   \n",
      "2 2011-12-19   3.50  0.511962           1.311777  1.473728   1.266368   \n",
      "3 2011-12-20   4.75  0.822695           3.461711  1.840524   3.831441   \n",
      "4 2011-12-21   4.38  0.753247           2.180044  1.969366   2.760808   \n",
      "\n",
      "   20D PMO  35D PMO        ADX  actions  y-hat  Unnamed: 11  Unnamed: 12  \n",
      "0    3.270     0.59  69.772809      NaN      2            1  2157.000000  \n",
      "1    3.690     0.46  70.038037      NaN      2            1     0.912437  \n",
      "2    4.275     0.67  70.475308      NaN      2            1          NaN  \n",
      "3    5.140     1.55  68.503186      NaN      2            1          NaN  \n",
      "4    5.910     1.28  66.671929      NaN      2            1          NaN  \n"
     ]
    }
   ],
   "source": [
    "dfz = pd.read_excel('BTC daily_database.xlsx', sheet_name=0)\n",
    "# testing the input is correct or not\n",
    "print(dfz.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomize sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffler(filename):\n",
    "  dfs = pd.read_excel(filename, sheet_name=0, header=0)\n",
    "  # return the pandas dataframe\n",
    "  return dfs.reindex(np.random.permutation(dfs.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date   Price       RSI  Stedev from 20 MA  MACD n.f  50MA_N.F.  \\\n",
      "637  2013-09-14  127.71  0.538379           0.985193  0.597817   1.513545   \n",
      "630  2013-09-07  116.56  0.607936           0.025270  1.161183   1.055986   \n",
      "903  2014-06-07  647.16  0.753273           1.020990  1.804149   1.896525   \n",
      "1416 2015-11-05  407.99  0.900601           2.384397  2.955001   3.382846   \n",
      "851  2014-04-16  523.10  0.561295           2.041632  0.012366  -0.283775   \n",
      "\n",
      "      20D PMO  35D PMO        ADX  actions  y-hat  Unnamed: 11  Unnamed: 12  \n",
      "637     8.175     0.96  19.959342      NaN      2            1          NaN  \n",
      "630     7.395     0.60  25.257798      NaN      2            1          NaN  \n",
      "903     5.850     1.09  12.899277      NaN      2            1          NaN  \n",
      "1416    7.545     1.63  43.580481      4.0      4            0          NaN  \n",
      "851    -6.740    -0.32  35.524512      NaN      2            1          NaN  \n"
     ]
    }
   ],
   "source": [
    "df = shuffler('BTC daily_database.xlsx')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputX shape: (7, 2363)\n",
      "number of training samples = 2363\n",
      "number of variables = 7\n",
      "[[  5.38378670e-01   6.07936112e-01   7.53273406e-01 ...,   5.25460455e-01\n",
      "    6.51394766e-01   4.86498589e-01]\n",
      " [  9.85193038e-01   2.52702731e-02   1.02099009e+00 ...,  -4.70876182e-02\n",
      "    4.83406126e-01   8.91445342e-02]\n",
      " [  5.97816648e-01   1.16118286e+00   1.80414933e+00 ...,  -1.06569874e+00\n",
      "    6.25257060e-01  -8.64232768e-01]\n",
      " ..., \n",
      " [  8.17500000e+00   7.39500000e+00   5.85000000e+00 ...,   1.85350000e+01\n",
      "   -5.79500000e+00   8.11000000e+00]\n",
      " [  9.60000000e-01   6.00000000e-01   1.09000000e+00 ...,   1.76000000e+00\n",
      "   -4.00000000e-02   5.00000000e-01]\n",
      " [  1.99593425e+01   2.52577978e+01   1.28992768e+01 ...,   2.07200569e+01\n",
      "    4.29602551e+01   5.42950847e+01]]\n",
      "(1, 2363)\n"
     ]
    }
   ],
   "source": [
    "inputX = df.iloc[:,2:9 ].as_matrix()\n",
    "inputX = inputX.T\n",
    "inputY = df.iloc[:, 10:11].as_matrix()\n",
    "inputY = inputY.T\n",
    "print(\"inputX shape: \" + str(inputX.shape))\n",
    "print(\"number of training samples = \"+ str(inputX.shape[1]))\n",
    "print(\"number of variables = \" + str(inputX.shape[0]))\n",
    "print(inputX)\n",
    "print(inputY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 2, ..., 2, 2, 2]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperating to train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX's shape(7, 1893)\n",
      "testX's shape(7, 470)\n",
      "trainX's shape(1, 1893)\n",
      "testY's shape(1, 470)\n"
     ]
    }
   ],
   "source": [
    "trainX = inputX[:, : -470]\n",
    "trainY = inputY[:, : -470]\n",
    "testX = inputX[:, -470: ]\n",
    "testY = inputY[:, -470: ]\n",
    "print(\"trainX's shape\" + str(trainX.shape))\n",
    "print(\"testX's shape\" + str(testX.shape))\n",
    "print(\"trainX's shape\" + str(trainY.shape))\n",
    "print(\"testY's shape\" + str(testY.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer size inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_x's shape = 7\n",
      "first layer(n_h)'s shape = 10\n",
      "n_y's output = 1\n"
     ]
    }
   ],
   "source": [
    "n_x = inputX.shape[0] # size of input layer\n",
    "n_h = 10\n",
    "n_y = inputY.shape[0]\n",
    "print(\"n_x's shape = \" + str(n_x))\n",
    "print(\"first layer(n_h)'s shape = \" + str(n_h))\n",
    "print(\"n_y's output = \" + str(n_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create placeholder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \n",
    "    #Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape = [n_x,None])\n",
    "    Y = tf.placeholder(tf.float32, shape = [n_y, 1, None])  \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = Tensor(\"Placeholder:0\", shape=(7, ?), dtype=float32)\n",
      "(7, ?)\n",
      "Y = Tensor(\"Placeholder_1:0\", shape=(1, 1, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X, Y = create_placeholders(n_x,n_y)\n",
    "print (\"X = \" + str(X))\n",
    "print(X.shape)\n",
    "print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_parameters(n_x, n_h):\n",
    "    #Initializes weight parameters to build a neural network with tensorflow  \n",
    "    #tf.set_random_seed()                              \n",
    "    W1 = tf.get_variable(\"W1\", [n_h,n_x], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    b1 = tf.get_variable(\"b1\", [n_h,1], initializer = tf.zeros_initializer())\n",
    "    W2 = tf.get_variable(\"W2\", [10, n_h], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    b2 = tf.get_variable(\"b2\", [10, 1], initializer = tf.zeros_initializer())\n",
    "    W3 = tf.get_variable(\"W3\", [25, 10], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    b3 = tf.get_variable(\"b3\", [25, 1], initializer = tf.zeros_initializer())\n",
    "    W4 = tf.get_variable(\"W4\", [10, 25], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    b4 = tf.get_variable(\"b4\", [10,1], initializer = tf.zeros_initializer())\n",
    "    W5 = tf.get_variable(\"W5\", [5, 10], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    b5 = tf.get_variable(\"b5\", [5,1], initializer = tf.zeros_initializer())\n",
    "    #W6 = tf.get_variable(\"W6\", [5, 10], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    #b6 = tf.get_variable(\"b6\", [5,1], initializer = tf.zeros_initializer())\n",
    "    #W7 = tf.get_variable(\"W7\", [5, 10], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    #b7 = tf.get_variable(\"b7\", [5,1], initializer = tf.zeros_initializer())\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3,\n",
    "                  \"W4\": W4,\n",
    "                  \"b4\": b4,\n",
    "                  \"W5\": W5,\n",
    "                  \"b5\": b5}\n",
    "                  #\"W6\": W6,\n",
    "                  #\"b6\": b6,\n",
    "                  #\"W7\": W7,\n",
    "                  #\"b7\": b7}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = <tf.Variable 'W1:0' shape=(10, 7) dtype=float32_ref>\n",
      "b1 = <tf.Variable 'b1:0' shape=(10, 1) dtype=float32_ref>\n",
      "W2 = <tf.Variable 'W2:0' shape=(10, 10) dtype=float32_ref>\n",
      "b2 = <tf.Variable 'b2:0' shape=(10, 1) dtype=float32_ref>\n",
      "W3 = <tf.Variable 'W3:0' shape=(25, 10) dtype=float32_ref>\n",
      "b3 = <tf.Variable 'b3:0' shape=(25, 1) dtype=float32_ref>\n",
      "W4 = <tf.Variable 'W4:0' shape=(10, 25) dtype=float32_ref>\n",
      "b4 = <tf.Variable 'b4:0' shape=(10, 1) dtype=float32_ref>\n",
      "W5 = <tf.Variable 'W5:0' shape=(5, 10) dtype=float32_ref>\n",
      "b5 = <tf.Variable 'b5:0' shape=(5, 1) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    parameters = initialize_parameters(n_x, n_h)\n",
    "    print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "    print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "    print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "    print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "    print(\"W3 = \" + str(parameters[\"W3\"]))\n",
    "    print(\"b3 = \" + str(parameters[\"b3\"]))\n",
    "    print(\"W4 = \" + str(parameters[\"W4\"]))\n",
    "    print(\"b4 = \" + str(parameters[\"b4\"]))\n",
    "    print(\"W5 = \" + str(parameters[\"W5\"]))\n",
    "    print(\"b5 = \" + str(parameters[\"b5\"]))\n",
    "    #print(\"W6 = \" + str(parameters[\"W6\"]))\n",
    "    #print(\"b6 = \" + str(parameters[\"b6\"]))\n",
    "    #print(\"W7 = \" + str(parameters[\"W7\"]))\n",
    "    #print(\"b7 = \" + str(parameters[\"b7\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_matrix(labels, C):\n",
    "    \"\"\"\n",
    "    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n",
    "                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n",
    "                     will be 1. \n",
    "                     \n",
    "    Arguments:\n",
    "    labels -- vector containing the labels \n",
    "    C -- number of classes, the depth of the one hot dimension\n",
    "    \n",
    "    Returns: \n",
    "    one_hot -- one hot matrix\n",
    "    \"\"\"\n",
    "    # Create a tf.constant equal to C (depth), name it 'C'. (approx. 1 line)\n",
    "    C = tf.constant(C, name = \"C\")\n",
    "    \n",
    "    # Use tf.one_hot, be careful with the axis (approx. 1 line)\n",
    "    one_hot_matrix = tf.one_hot(indices = labels, depth = C,axis  =0)\n",
    "    \n",
    "    # Create the session (approx. 1 line)\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    # Run the session (approx. 1 line)\n",
    "    one_hot = sess.run(one_hot_matrix)\n",
    "    \n",
    "    # Close the session (approx. 1 line). See method 1 above.\n",
    "    sess.close()\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot = [[ 0.  0.  0.  1.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([1,2,3,0,2,1])\n",
    "one_hot = one_hot_matrix(labels, C = 4)\n",
    "print (\"one_hot = \" + str(one_hot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Y-hat to softmax matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y = \n",
      "[[[ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 1.  1.  1. ...,  1.  1.  1.]]\n",
      "\n",
      " [[ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ...,  0.  0.  0.]]]\n",
      "trainY-shape = (5, 1, 1893)\n",
      "testY-shape = (5, 1, 470)\n"
     ]
    }
   ],
   "source": [
    "trainY = one_hot_matrix(trainY, C = 5)\n",
    "testY = one_hot_matrix(testY, C = 5) \n",
    "print(\"Y = \")\n",
    "print(trainY)\n",
    "print(\"trainY-shape = \" + str(trainY.shape))\n",
    "print(\"testY-shape = \" + str(testY.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\", \"W4\", \"b4\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z4 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    W4 = parameters['W4']\n",
    "    b4 = parameters['b4']\n",
    "    W5 = parameters['W5']\n",
    "    b5 = parameters['b5'] \n",
    "    #W6 = parameters['W6']\n",
    "    #b6 = parameters['b6'] \n",
    "    #W7 = parameters['W7']\n",
    "    #b7 = parameters['b7'] \n",
    "                                                                     \n",
    "    Z1 = tf.add(tf.matmul(W1,X) , b1 )                              \n",
    "    A1 = tf.nn.tanh(Z1)                                                  \n",
    "    Z2 = tf.add(tf.matmul(W2,A1), b2 )                               \n",
    "    A2 = tf.nn.sigmoid(Z2)                                                  \n",
    "    Z3 = tf.add(tf.matmul(W3,A2), b3)\n",
    "    A3 = tf.nn.selu(Z3)                                         \n",
    "    Z4 = tf.add(tf.matmul(W4,A3), b4)                                \n",
    "    A4 = tf.nn.selu(Z4)                                                  \n",
    "    Z5 = tf.add(tf.matmul(W5,A4), b5)                                 \n",
    "    #A5 = tf.nn.tanh(Z5)                                              \n",
    "    #Z6 = tf.add(tf.matmul(W6,A5), b6)                                 \n",
    "    #A6 = tf.nn.relu(Z6)                                              \n",
    "    #Z7 = tf.add(tf.matmul(W7,A6), b7)                                \n",
    "    return Z5\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z4 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z4\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    #compute cost\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-237d1ee019a2>:18: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "cost = Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(4, 1)\n",
    "    parameters = initialize_parameters(4,1)\n",
    "    Z1 = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(Z1, Y)\n",
    "    print(\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "\n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    #permutation = list(np.random.permutation(m))\n",
    "    #shuffled_X = X[:, permutation]\n",
    "    #shuffled_Y = Y[:, 1, permutation]\n",
    "    \n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = X[:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "        mini_batch_Y = Y[:,:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = X[:, num_complete_minibatches*mini_batch_size : m]\n",
    "        mini_batch_Y = Y[:,:, num_complete_minibatches*mini_batch_size : m]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the 1st mini_batch_X: (7, 64)\n",
      "shape of the 2nd mini_batch_X: (7, 64)\n",
      "shape of the 3rd mini_batch_X: (7, 64)\n",
      "shape of the last mini_batch_X: (7, 37)\n",
      "shape of the 1st mini_batch_Y: (5, 1, 64)\n",
      "shape of the 2nd mini_batch_Y: (5, 1, 64)\n",
      "shape of the 3rd mini_batch_Y: (5, 1, 64)\n",
      "shape of the last mini_batch_Y: (5, 1, 37)\n"
     ]
    }
   ],
   "source": [
    "mini_batches = random_mini_batches(trainX, trainY, 64, seed=0)\n",
    "\n",
    "print (\"shape of the 1st mini_batch_X: \" + str(mini_batches[0][0].shape))\n",
    "print (\"shape of the 2nd mini_batch_X: \" + str(mini_batches[1][0].shape))\n",
    "print (\"shape of the 3rd mini_batch_X: \" + str(mini_batches[2][0].shape))\n",
    "print (\"shape of the last mini_batch_X: \" + str(mini_batches[-1][0].shape))\n",
    "print (\"shape of the 1st mini_batch_Y: \" + str(mini_batches[0][1].shape))\n",
    "print (\"shape of the 2nd mini_batch_Y: \" + str(mini_batches[1][1].shape)) \n",
    "print (\"shape of the 3rd mini_batch_Y: \" + str(mini_batches[2][1].shape))\n",
    "print (\"shape of the last mini_batch_Y: \" + str(mini_batches[-1][1].shape))\n",
    "#print (\"mini batch sanity check: \")\n",
    "#mini_batches[0][1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(trainX, trainY, testX, testY, learning_rate = 0.0013,\n",
    "          num_epochs = 2200, minibatch_size = 64, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = trainX.shape                                # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = trainY.shape[0]                                  # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_x, n_h)\n",
    "\n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    Z = forward_propagation(X, parameters)\n",
    "    \n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(Z,Y)\n",
    "\n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "\n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(trainX, trainY, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "\n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "\n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: trainX, Y: trainY}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: testX, Y: testY}))\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.657554\n",
      "Cost after epoch 100: 0.276212\n",
      "Cost after epoch 200: 0.263797\n",
      "Cost after epoch 300: 0.254662\n",
      "Cost after epoch 400: 0.244347\n",
      "Cost after epoch 500: 0.236940\n",
      "Cost after epoch 600: 0.231903\n",
      "Cost after epoch 700: 0.227545\n",
      "Cost after epoch 800: 0.224714\n",
      "Cost after epoch 900: 0.221607\n",
      "Cost after epoch 1000: 0.218643\n",
      "Cost after epoch 1100: 0.216114\n",
      "Cost after epoch 1200: 0.213033\n",
      "Cost after epoch 1300: 0.209917\n",
      "Cost after epoch 1400: 0.205995\n",
      "Cost after epoch 1500: 0.202546\n",
      "Cost after epoch 1600: 0.199584\n",
      "Cost after epoch 1700: 0.197012\n",
      "Cost after epoch 1800: 0.193589\n",
      "Cost after epoch 1900: 0.191848\n",
      "Cost after epoch 2000: 0.189634\n",
      "Cost after epoch 2100: 0.187937\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n",
      "Train Accuracy: 0.937665\n",
      "Test Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "# run the model \n",
    "parameters = model(trainX, trainY, testX,testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
