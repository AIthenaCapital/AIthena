{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dependencies \n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date  Price       RSI  Stedev from 20 MA  MACD n.f  50MA_N.F.  \\\n",
      "0 2011-12-16   3.25  0.514151           0.591527  1.594689   0.500194   \n",
      "1 2011-12-18   3.25  0.443243           0.516482  1.489778   0.571086   \n",
      "2 2011-12-19   3.50  0.511962           1.311777  1.473728   1.266368   \n",
      "3 2011-12-20   4.75  0.822695           3.461711  1.840524   3.831441   \n",
      "4 2011-12-21   4.38  0.753247           2.180044  1.969366   2.760808   \n",
      "\n",
      "   20D PMO  35D PMO        ADX  actions  y-hat  Unnamed: 11  Unnamed: 12  \n",
      "0    3.270     0.59  69.772809      NaN      2            1  2157.000000  \n",
      "1    3.690     0.46  70.038037      NaN      2            1     0.912437  \n",
      "2    4.275     0.67  70.475308      NaN      2            1          NaN  \n",
      "3    5.140     1.55  68.503186      NaN      2            1          NaN  \n",
      "4    5.910     1.28  66.671929      NaN      2            1          NaN  \n"
     ]
    }
   ],
   "source": [
    "# importing data using pandas \n",
    "dfz = pd.read_excel('BTC daily_database.xlsx', sheet_name=0)\n",
    "# testing the input is correct or not\n",
    "print(dfz.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomize sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffler() will read the data and randomly organizes it \n",
    "def shuffler(filename):\n",
    "  # Read an Excel table into a pandas DataFrame\n",
    "  # takes the first sheet and header\n",
    "    \n",
    "  dfs = pd.read_excel(filename, sheet_name=0, header=0)\n",
    "  # return the pandas dataframe\n",
    "  # np.random.permutation shuffles the data points \n",
    "  # dfs will clean the data by placing NaN at x-features that have no data \n",
    "  return dfs.reindex(np.random.permutation(dfs.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date    Price       RSI  Stedev from 20 MA  MACD n.f  50MA_N.F.  \\\n",
      "98   2012-03-24     4.72  0.441718          -1.372787 -0.172724  -0.864722   \n",
      "828  2014-03-24   558.22  0.164125          -1.995781 -0.159128  -1.200441   \n",
      "2137 2017-10-26  5741.35  0.684951           0.618055  0.796806   1.450945   \n",
      "206  2012-07-10     6.95  0.789916           2.373414  0.376840   1.602152   \n",
      "176  2012-06-10     5.54  0.859375           1.878567  2.907242   2.878015   \n",
      "\n",
      "      20D PMO  35D PMO        ADX  actions  y-hat  Unnamed: 11  Unnamed: 12  \n",
      "98     -1.690     0.11  24.690947      NaN      2            1          NaN  \n",
      "828    -2.995    -0.19  41.472985      NaN      2            1          NaN  \n",
      "2137    8.795     1.21  22.638714      NaN      2            1          NaN  \n",
      "206     7.555     0.85  14.373037      NaN      2            1          NaN  \n",
      "176     0.780     0.27  26.943206      NaN      2            1          NaN  \n"
     ]
    }
   ],
   "source": [
    "# randomizes the data and saves it in df\n",
    "df = shuffler('BTC daily_database.xlsx')\n",
    "\n",
    "# prints the first 5 rows of the data table\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputX shape: (7, 2363)\n",
      "number of training samples = 2363\n",
      "number of variables = 7\n",
      "[[  4.41717791e-01   1.64125335e-01   6.84951073e-01 ...,   8.15602837e-01\n",
      "    8.48794226e-01   5.94053119e-01]\n",
      " [ -1.37278699e+00  -1.99578130e+00   6.18054525e-01 ...,   2.67979882e+00\n",
      "    2.29682429e+00   9.77566896e-01]\n",
      " [ -1.72723616e-01  -1.59127857e-01   7.96805614e-01 ...,   3.25286892e-01\n",
      "    3.12572020e+00   1.16546495e+00]\n",
      " ..., \n",
      " [ -1.69000000e+00  -2.99500000e+00   8.79500000e+00 ...,  -2.98500000e+00\n",
      "    1.22500000e+00   1.23300000e+01]\n",
      " [  1.10000000e-01  -1.90000000e-01   1.21000000e+00 ...,  -5.00000000e-02\n",
      "    3.30000000e-01   1.08000000e+00]\n",
      " [  2.46909473e+01   4.14729851e+01   2.26387138e+01 ...,   5.60678609e+01\n",
      "    1.75089142e+01   1.32235335e+01]]\n",
      "(1, 2363)\n"
     ]
    }
   ],
   "source": [
    "# converts to numpy array using as_matrix()\n",
    "# use iloc() to get location \n",
    "inputX = df.iloc[:,2:9 ].as_matrix()\n",
    "\n",
    "# transposes inputX\n",
    "inputX = inputX.T\n",
    "\n",
    "# converts to numpy array using as_matrix()\n",
    "# use iloc() to get location \n",
    "inputY = df.iloc[:, 10:11].as_matrix()\n",
    "\n",
    "# transposes inputY\n",
    "inputY = inputY.T\n",
    "\n",
    "print(\"inputX shape: \" + str(inputX.shape))\n",
    "print(\"number of training samples = \"+ str(inputX.shape[1]))\n",
    "print(\"number of variables = \" + str(inputX.shape[0]))\n",
    "print(inputX)\n",
    "print(inputY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 2, ..., 2, 3, 2]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperating to train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX's shape: (7, 1893)\n",
      "testX's shape: (7, 470)\n",
      "trainX's shape: (1, 1893)\n",
      "testY's shape: (1, 470)\n"
     ]
    }
   ],
   "source": [
    "# selects trainning data \n",
    "trainX = inputX[:, : -470]\n",
    "trainY = inputY[:, : -470]\n",
    "\n",
    "# selects test data \n",
    "testX = inputX[:, -470: ]\n",
    "testY = inputY[:, -470: ]\n",
    "\n",
    "# verifies the shapes \n",
    "print(\"trainX's shape: \" + str(trainX.shape))\n",
    "print(\"testX's shape: \" + str(testX.shape))\n",
    "print(\"trainX's shape: \" + str(trainY.shape))\n",
    "print(\"testY's shape: \" + str(testY.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer size inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_x's shape = 7\n",
      "first layer(n_h)'s shape = 10\n",
      "n_y's output = 1\n"
     ]
    }
   ],
   "source": [
    "# size of input layer\n",
    "n_x = inputX.shape[0]\n",
    "\n",
    "# amount of hidden layers\n",
    "n_h = 10\n",
    "\n",
    "# output layer \n",
    "n_y = inputY.shape[0]\n",
    "\n",
    "print(\"n_x's shape = \" + str(n_x))\n",
    "print(\"first layer(n_h)'s shape = \" + str(n_h))\n",
    "print(\"n_y's output = \" + str(n_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create placeholder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the placeholders for the tensorflow session.\n",
    "def create_placeholders(n_x, n_y):\n",
    "    \n",
    "    # creates placholder for x and y that batch size can be any size\n",
    "    X = tf.placeholder(tf.float32, shape = [n_x,None])\n",
    "    Y = tf.placeholder(tf.float32, shape = [n_y, 1, None])  \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = Tensor(\"Placeholder:0\", shape=(7, ?), dtype=float32)\n",
      "(7, ?)\n",
      "Y = Tensor(\"Placeholder_1:0\", shape=(1, 1, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X, Y = create_placeholders(n_x,n_y)\n",
    "print (\"X = \" + str(X))\n",
    "print(X.shape)\n",
    "print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Initializes weight parameters to build a neural network with tensorflow  \n",
    "def initialize_parameters(n_x, n_h):\n",
    "    #tf.set_random_seed()\n",
    "    \n",
    "    # Layer 1 (Input Layer)\n",
    "    W1 = tf.get_variable(\"W1\", [n_h,n_x], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    b1 = tf.get_variable(\"b1\", [n_h,1], initializer = tf.zeros_initializer())\n",
    "    \n",
    "    # Layer 2 (10 neurons)\n",
    "    W2 = tf.get_variable(\"W2\", [10, n_h], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    b2 = tf.get_variable(\"b2\", [10, 1], initializer = tf.zeros_initializer())\n",
    "    \n",
    "    # Layer 3 (25 neurons)\n",
    "    W3 = tf.get_variable(\"W3\", [25, 10], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    b3 = tf.get_variable(\"b3\", [25, 1], initializer = tf.zeros_initializer())\n",
    "    \n",
    "    # Layer 4 (10 neurons)\n",
    "    W4 = tf.get_variable(\"W4\", [10, 25], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    b4 = tf.get_variable(\"b4\", [10,1], initializer = tf.zeros_initializer())\n",
    "    \n",
    "    # Layer 5 (5 neurons)\n",
    "    W5 = tf.get_variable(\"W5\", [5, 10], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    b5 = tf.get_variable(\"b5\", [5,1], initializer = tf.zeros_initializer())\n",
    "    \n",
    "    #W6 = tf.get_variable(\"W6\", [5, 10], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    #b6 = tf.get_variable(\"b6\", [5,1], initializer = tf.zeros_initializer())\n",
    "    #W7 = tf.get_variable(\"W7\", [5, 10], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    #b7 = tf.get_variable(\"b7\", [5,1], initializer = tf.zeros_initializer())\n",
    "    \n",
    "    # saves weights and bias for layers as a dictionary \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3,\n",
    "                  \"W4\": W4,\n",
    "                  \"b4\": b4,\n",
    "                  \"W5\": W5,\n",
    "                  \"b5\": b5}\n",
    "                  #\"W6\": W6,\n",
    "                  #\"b6\": b6,\n",
    "                  #\"W7\": W7,\n",
    "                  #\"b7\": b7}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = <tf.Variable 'W1:0' shape=(10, 7) dtype=float32_ref>\n",
      "b1 = <tf.Variable 'b1:0' shape=(10, 1) dtype=float32_ref>\n",
      "W2 = <tf.Variable 'W2:0' shape=(10, 10) dtype=float32_ref>\n",
      "b2 = <tf.Variable 'b2:0' shape=(10, 1) dtype=float32_ref>\n",
      "W3 = <tf.Variable 'W3:0' shape=(25, 10) dtype=float32_ref>\n",
      "b3 = <tf.Variable 'b3:0' shape=(25, 1) dtype=float32_ref>\n",
      "W4 = <tf.Variable 'W4:0' shape=(10, 25) dtype=float32_ref>\n",
      "b4 = <tf.Variable 'b4:0' shape=(10, 1) dtype=float32_ref>\n",
      "W5 = <tf.Variable 'W5:0' shape=(5, 10) dtype=float32_ref>\n",
      "b5 = <tf.Variable 'b5:0' shape=(5, 1) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "# Checking the shape of the weights and bias \n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    parameters = initialize_parameters(n_x, n_h)\n",
    "    print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "    print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "    print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "    print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "    print(\"W3 = \" + str(parameters[\"W3\"]))\n",
    "    print(\"b3 = \" + str(parameters[\"b3\"]))\n",
    "    print(\"W4 = \" + str(parameters[\"W4\"]))\n",
    "    print(\"b4 = \" + str(parameters[\"b4\"]))\n",
    "    print(\"W5 = \" + str(parameters[\"W5\"]))\n",
    "    print(\"b5 = \" + str(parameters[\"b5\"]))\n",
    "    #print(\"W6 = \" + str(parameters[\"W6\"]))\n",
    "    #print(\"b6 = \" + str(parameters[\"b6\"]))\n",
    "    #print(\"W7 = \" + str(parameters[\"W7\"]))\n",
    "    #print(\"b7 = \" + str(parameters[\"b7\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_matrix(labels, C):\n",
    "    \"\"\"\n",
    "    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n",
    "                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n",
    "                     will be 1. \n",
    "                     \n",
    "    Arguments:\n",
    "    labels -- vector containing the labels \n",
    "    C -- number of classes, the depth of the one hot dimension\n",
    "    \n",
    "    Returns: \n",
    "    one_hot -- one hot matrix\n",
    "    \"\"\"\n",
    "    # Create a tf.constant equal to C (depth), name it 'C'. (approx. 1 line)\n",
    "    C = tf.constant(C, name = \"C\")\n",
    "    \n",
    "    # Use tf.one_hot, be careful with the axis (approx. 1 line)\n",
    "    one_hot_matrix = tf.one_hot(indices = labels, depth = C,axis  =0)\n",
    "    \n",
    "    # Create the session (approx. 1 line)\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    # Run the session (approx. 1 line)\n",
    "    one_hot = sess.run(one_hot_matrix)\n",
    "    \n",
    "    # Close the session (approx. 1 line). See method 1 above.\n",
    "    sess.close()\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot = [[ 0.  0.  0.  1.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Testing one_hot\n",
    "labels = np.array([1,2,3,0,2,1])\n",
    "one_hot = one_hot_matrix(labels, C = 4)\n",
    "print (\"one_hot = \" + str(one_hot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Y-hat to softmax matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y = \n",
      "[[[ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 1.  1.  1. ...,  1.  1.  1.]]\n",
      "\n",
      " [[ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ...,  0.  0.  0.]]]\n",
      "trainY-shape = (5, 1, 1893)\n",
      "testY-shape = (5, 1, 470)\n"
     ]
    }
   ],
   "source": [
    "# use one_hot on trainY and testY\n",
    "trainY = one_hot_matrix(trainY, C = 5)\n",
    "testY = one_hot_matrix(testY, C = 5)\n",
    "\n",
    "# bug ? \n",
    "print(\"Y = \")\n",
    "print(trainY)\n",
    "\n",
    "print(\"trainY-shape = \" + str(trainY.shape))\n",
    "print(\"testY-shape = \" + str(testY.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model:\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\", \"W4\", \"b4\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z4 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    W4 = parameters['W4']\n",
    "    b4 = parameters['b4']\n",
    "    W5 = parameters['W5']\n",
    "    b5 = parameters['b5'] \n",
    "    #W6 = parameters['W6']\n",
    "    #b6 = parameters['b6'] \n",
    "    #W7 = parameters['W7']\n",
    "    #b7 = parameters['b7'] \n",
    "    \n",
    "    # Input Layer (Sigmoid)\n",
    "    Z1 = tf.add(tf.matmul(W1,X) , b1 )                              \n",
    "    A1 = tf.nn.tanh(Z1)\n",
    "    \n",
    "    # 1st Hidden Layer (Sigmoid)\n",
    "    Z2 = tf.add(tf.matmul(W2,A1), b2 )                               \n",
    "    A2 = tf.nn.sigmoid(Z2)\n",
    "    \n",
    "    # 2nd Hidden Layer (Selu)\n",
    "    Z3 = tf.add(tf.matmul(W3,A2), b3)\n",
    "    A3 = tf.nn.selu(Z3) \n",
    "    \n",
    "    # 3rd Hidden Layer (Selu)\n",
    "    Z4 = tf.add(tf.matmul(W4,A3), b4)                                \n",
    "    A4 = tf.nn.selu(Z4)             \n",
    "    \n",
    "    # Output Layer\n",
    "    Z5 = tf.add(tf.matmul(W5,A4), b5)                                 \n",
    "    #A5 = tf.nn.tanh(Z5)                                              \n",
    "    #Z6 = tf.add(tf.matmul(W6,A5), b6)                                 \n",
    "    #A6 = tf.nn.relu(Z6)                                              \n",
    "    #Z7 = tf.add(tf.matmul(W7,A6), b7)                                \n",
    "    return Z5\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z4 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z4\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    #compute cost\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-237d1ee019a2>:18: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "cost = Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(4, 1)\n",
    "    parameters = initialize_parameters(4,1)\n",
    "    Z1 = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(Z1, Y)\n",
    "    print(\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "\n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    #permutation = list(np.random.permutation(m))\n",
    "    #shuffled_X = X[:, permutation]\n",
    "    #shuffled_Y = Y[:, 1, permutation]\n",
    "    \n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = X[:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "        mini_batch_Y = Y[:,:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = X[:, num_complete_minibatches*mini_batch_size : m]\n",
    "        mini_batch_Y = Y[:,:, num_complete_minibatches*mini_batch_size : m]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the 1st mini_batch_X: (7, 64)\n",
      "shape of the 2nd mini_batch_X: (7, 64)\n",
      "shape of the 3rd mini_batch_X: (7, 64)\n",
      "shape of the last mini_batch_X: (7, 37)\n",
      "shape of the 1st mini_batch_Y: (5, 1, 64)\n",
      "shape of the 2nd mini_batch_Y: (5, 1, 64)\n",
      "shape of the 3rd mini_batch_Y: (5, 1, 64)\n",
      "shape of the last mini_batch_Y: (5, 1, 37)\n"
     ]
    }
   ],
   "source": [
    "mini_batches = random_mini_batches(trainX, trainY, 64, seed=0)\n",
    "\n",
    "print (\"shape of the 1st mini_batch_X: \" + str(mini_batches[0][0].shape))\n",
    "print (\"shape of the 2nd mini_batch_X: \" + str(mini_batches[1][0].shape))\n",
    "print (\"shape of the 3rd mini_batch_X: \" + str(mini_batches[2][0].shape))\n",
    "print (\"shape of the last mini_batch_X: \" + str(mini_batches[-1][0].shape))\n",
    "print (\"shape of the 1st mini_batch_Y: \" + str(mini_batches[0][1].shape))\n",
    "print (\"shape of the 2nd mini_batch_Y: \" + str(mini_batches[1][1].shape)) \n",
    "print (\"shape of the 3rd mini_batch_Y: \" + str(mini_batches[2][1].shape))\n",
    "print (\"shape of the last mini_batch_Y: \" + str(mini_batches[-1][1].shape))\n",
    "#print (\"mini batch sanity check: \")\n",
    "#mini_batches[0][1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(trainX, trainY, testX, testY, learning_rate = 0.0013,\n",
    "          num_epochs = 2200, minibatch_size = 64, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = trainX.shape                                # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = trainY.shape[0]                                  # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_x, n_h)\n",
    "\n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    Z = forward_propagation(X, parameters)\n",
    "    \n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(Z,Y)\n",
    "\n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "\n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(trainX, trainY, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "\n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "\n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: trainX, Y: trainY}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: testX, Y: testY}))\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.653183\n",
      "Cost after epoch 100: 0.276426\n",
      "Cost after epoch 200: 0.260272\n",
      "Cost after epoch 300: 0.253854\n",
      "Cost after epoch 400: 0.249470\n",
      "Cost after epoch 500: 0.246521\n",
      "Cost after epoch 600: 0.242008\n",
      "Cost after epoch 700: 0.238269\n",
      "Cost after epoch 800: 0.233557\n",
      "Cost after epoch 900: 0.229125\n",
      "Cost after epoch 1000: 0.224712\n",
      "Cost after epoch 1100: 0.220482\n",
      "Cost after epoch 1200: 0.217162\n",
      "Cost after epoch 1300: 0.214317\n",
      "Cost after epoch 1400: 0.212292\n",
      "Cost after epoch 1500: 0.209196\n",
      "Cost after epoch 1600: 0.207171\n",
      "Cost after epoch 1700: 0.202495\n",
      "Cost after epoch 1800: 0.199362\n",
      "Cost after epoch 1900: 0.199863\n",
      "Cost after epoch 2000: 0.192701\n",
      "Cost after epoch 2100: 0.189174\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n",
      "Train Accuracy: 0.93608\n",
      "Test Accuracy: 0.904255\n"
     ]
    }
   ],
   "source": [
    "# run the model \n",
    "parameters = model(trainX, trainY, testX,testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
