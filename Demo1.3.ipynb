{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date  Price       RSI  Stedev from 20 MA  MACD n.f  50MA_N.F.  \\\n",
      "0 2011-12-16   3.25  0.514151           0.591527  1.594689   0.500194   \n",
      "1 2011-12-18   3.25  0.443243           0.516482  1.489778   0.571086   \n",
      "2 2011-12-19   3.50  0.511962           1.311777  1.473728   1.266368   \n",
      "3 2011-12-20   4.75  0.822695           3.461711  1.840524   3.831441   \n",
      "4 2011-12-21   4.38  0.753247           2.180044  1.969366   2.760808   \n",
      "\n",
      "   actions  y-hat  Unnamed: 8   Unnamed: 9  \n",
      "0      NaN      2           1  2157.000000  \n",
      "1      NaN      2           1     0.912437  \n",
      "2      NaN      2           1          NaN  \n",
      "3      NaN      2           1          NaN  \n",
      "4      NaN      2           1          NaN  \n"
     ]
    }
   ],
   "source": [
    "dfz = pd.read_excel('BTC daily_database.xlsx', sheet_name=0)\n",
    "# testing the input is correct or not\n",
    "print(dfz.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomize sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffler(filename):\n",
    "  dfs = pd.read_excel(filename, sheet_name=0, header=0)\n",
    "  # return the pandas dataframe\n",
    "  return dfs.reindex(np.random.permutation(dfs.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date    Price       RSI  Stedev from 20 MA  MACD n.f  50MA_N.F.  \\\n",
      "1992 2017-06-03  2488.94  0.680345           1.572151  1.167032   1.957879   \n",
      "1280 2015-06-22   243.60  0.771303           1.091509  2.456207   1.267424   \n",
      "2149 2017-11-07  6967.64  0.683866           1.148204  1.116497   1.628582   \n",
      "1067 2014-11-18   388.72  0.673406           0.915072  1.519611   0.931693   \n",
      "1385 2015-10-05   238.60  0.636568           1.295540  0.734069   0.738402   \n",
      "\n",
      "      actions  y-hat  Unnamed: 8  Unnamed: 9  \n",
      "1992      NaN      2           1         NaN  \n",
      "1280      NaN      2           1         NaN  \n",
      "2149      NaN      2           1         NaN  \n",
      "1067      NaN      2           1         NaN  \n",
      "1385      NaN      2           1         NaN  \n"
     ]
    }
   ],
   "source": [
    "df = shuffler('BTC daily_database.xlsx')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (4, 2363)\n",
      "number of training samples = 2363\n",
      "number of variables = 4\n",
      "[[ 0.680345    0.77130277  0.68386604 ...  0.70123144  0.69871795\n",
      "   0.42354248]\n",
      " [ 1.57215123  1.09150886  1.14820388 ...  1.19100035  1.21857915\n",
      "  -0.13331296]\n",
      " [ 1.16703172  2.45620738  1.11649711 ...  0.08804087  0.13067132\n",
      "   0.80795293]\n",
      " [ 1.95787908  1.26742422  1.62858206 ...  1.60517657  0.76437971\n",
      "   0.34977224]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\john liu\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\john liu\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "inputX = df.iloc[:,2:6 ].as_matrix()\n",
    "inputX = inputX.T\n",
    "inputY = df.iloc[:, 7:8].as_matrix()\n",
    "inputY = inputY.T\n",
    "print(\"X_train shape: \" + str(inputX.shape))\n",
    "print(\"number of training samples = \"+ str(inputX.shape[1]))\n",
    "print(\"number of variables = \" + str(inputX.shape[0]))\n",
    "print(inputX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 2, ..., 2, 2, 2]], dtype=int64)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperating to train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2213)\n",
      "(4, 150)\n",
      "(1, 150)\n",
      "[[ 0.42831849  0.91754757  0.67282717  0.47941709  0.3462651   0.19705268\n",
      "   0.6249251   0.71521197  0.32656624  0.84482143]\n",
      " [-1.30284002  1.39972865  0.31029031 -0.76597023 -0.92823343 -1.87590366\n",
      "   1.64628939  0.95668089 -0.63477071  1.15445844]\n",
      " [-2.33192578  2.35449977  0.6826062  -0.99752716 -1.17926572 -1.34191947\n",
      "   0.57704406  1.56521121 -1.2210259   1.50200195]\n",
      " [-0.75757415  2.3940322   0.39701356 -0.13679561 -1.42330937 -2.56270409\n",
      "   1.25947507  1.79808133 -1.50333185  1.87020068]]\n"
     ]
    }
   ],
   "source": [
    "trainX = inputX[:, : -150]\n",
    "trainY = inputY[:, : -150]\n",
    "testX = inputX[:, -150: ]\n",
    "testY = inputY[:, -150: ]\n",
    "print(trainX.shape)\n",
    "print(testX.shape)\n",
    "print(testY.shape)\n",
    "print(testX[:,: 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer size inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_x's shape = 4\n",
      "first layer(n_h)'s shape = 5\n",
      "n_y's output = 1\n"
     ]
    }
   ],
   "source": [
    "n_x = inputX.shape[0] # size of input layer\n",
    "n_h = 5\n",
    "n_y = inputY.shape[0]\n",
    "print(\"n_x's shape = \" + str(n_x))\n",
    "print(\"first layer(n_h)'s shape = \" + str(n_h))\n",
    "print(\"n_y's output = \" + str(n_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create placeholder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \n",
    "    #Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape = [n_x,None])\n",
    "    Y = tf.placeholder(tf.float32, shape = [n_y, 1, None])  \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = Tensor(\"Placeholder_2:0\", shape=(4, ?), dtype=float32)\n",
      "(4, ?)\n",
      "Y = Tensor(\"Placeholder_3:0\", shape=(1, 1, ?), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "X, Y = create_placeholders(n_x,n_y)\n",
    "print (\"X = \" + str(X))\n",
    "print(X.shape)\n",
    "print (\"Y = \" + str(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_parameters(n_x, n_h):\n",
    "    #Initializes weight parameters to build a neural network with tensorflow  \n",
    "    #tf.set_random_seed()                              \n",
    "    W1 = tf.get_variable(\"W1\", [n_h,n_x], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    b1 = tf.get_variable(\"b1\", [n_h,1], initializer = tf.zeros_initializer())\n",
    "    W2 = tf.get_variable(\"W2\", [10, n_h], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    b2 = tf.get_variable(\"b2\", [10, 1], initializer = tf.zeros_initializer())\n",
    "    W3 = tf.get_variable(\"W3\", [10, 10], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    b3 = tf.get_variable(\"b3\", [10, 1], initializer = tf.zeros_initializer())\n",
    "    W4 = tf.get_variable(\"W4\", [10, 10], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    b4 = tf.get_variable(\"b4\", [10,1], initializer = tf.zeros_initializer())\n",
    "    W5 = tf.get_variable(\"W5\", [10, 10], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    b5 = tf.get_variable(\"b5\", [10,1], initializer = tf.zeros_initializer())\n",
    "    W6 = tf.get_variable(\"W6\", [10, 10], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    b6 = tf.get_variable(\"b6\", [10,1], initializer = tf.zeros_initializer())\n",
    "    W7= tf.get_variable(\"W7\", [5, 10], initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    b7 = tf.get_variable(\"b7\", [5,1], initializer = tf.zeros_initializer())\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3,\n",
    "                  \"W4\": W4,\n",
    "                  \"b4\": b4,\n",
    "                  \"W5\": W5,\n",
    "                  \"b5\": b5,\n",
    "                  \"W6\": W6,\n",
    "                  \"b6\": b6,\n",
    "                  \"W7\": W7,\n",
    "                  \"b7\": b7}\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = <tf.Variable 'W1:0' shape=(5, 4) dtype=float32_ref>\n",
      "b1 = <tf.Variable 'b1:0' shape=(5, 1) dtype=float32_ref>\n",
      "W2 = <tf.Variable 'W2:0' shape=(10, 5) dtype=float32_ref>\n",
      "b2 = <tf.Variable 'b2:0' shape=(10, 1) dtype=float32_ref>\n",
      "W3 = <tf.Variable 'W3:0' shape=(10, 10) dtype=float32_ref>\n",
      "b3 = <tf.Variable 'b3:0' shape=(10, 1) dtype=float32_ref>\n",
      "W4 = <tf.Variable 'W4:0' shape=(10, 10) dtype=float32_ref>\n",
      "b4 = <tf.Variable 'b4:0' shape=(10, 1) dtype=float32_ref>\n",
      "W5 = <tf.Variable 'W5:0' shape=(10, 10) dtype=float32_ref>\n",
      "b5 = <tf.Variable 'b5:0' shape=(10, 1) dtype=float32_ref>\n",
      "W6 = <tf.Variable 'W6:0' shape=(10, 10) dtype=float32_ref>\n",
      "b6 = <tf.Variable 'b6:0' shape=(10, 1) dtype=float32_ref>\n",
      "W7 = <tf.Variable 'W7:0' shape=(5, 10) dtype=float32_ref>\n",
      "b7 = <tf.Variable 'b7:0' shape=(5, 1) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    parameters = initialize_parameters(n_x, n_h)\n",
    "    print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "    print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "    print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "    print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "    print(\"W3 = \" + str(parameters[\"W3\"]))\n",
    "    print(\"b3 = \" + str(parameters[\"b3\"]))\n",
    "    print(\"W4 = \" + str(parameters[\"W4\"]))\n",
    "    print(\"b4 = \" + str(parameters[\"b4\"]))\n",
    "    print(\"W5 = \" + str(parameters[\"W5\"]))\n",
    "    print(\"b5 = \" + str(parameters[\"b5\"]))\n",
    "    print(\"W6 = \" + str(parameters[\"W6\"]))\n",
    "    print(\"b6 = \" + str(parameters[\"b6\"]))\n",
    "    print(\"W7 = \" + str(parameters[\"W7\"]))\n",
    "    print(\"b7 = \" + str(parameters[\"b7\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_matrix(labels, C):\n",
    "    \"\"\"\n",
    "    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n",
    "                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n",
    "                     will be 1. \n",
    "                     \n",
    "    Arguments:\n",
    "    labels -- vector containing the labels \n",
    "    C -- number of classes, the depth of the one hot dimension\n",
    "    \n",
    "    Returns: \n",
    "    one_hot -- one hot matrix\n",
    "    \"\"\"\n",
    "    # Create a tf.constant equal to C (depth), name it 'C'. (approx. 1 line)\n",
    "    C = tf.constant(C, name = \"C\")\n",
    "    \n",
    "    # Use tf.one_hot, be careful with the axis (approx. 1 line)\n",
    "    one_hot_matrix = tf.one_hot(indices = labels, depth = C,axis  =0)\n",
    "    \n",
    "    # Create the session (approx. 1 line)\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    # Run the session (approx. 1 line)\n",
    "    one_hot = sess.run(one_hot_matrix)\n",
    "    \n",
    "    # Close the session (approx. 1 line). See method 1 above.\n",
    "    sess.close()\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_hot = [[0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([1,2,3,0,2,1])\n",
    "one_hot = one_hot_matrix(labels, C = 4)\n",
    "print (\"one_hot = \" + str(one_hot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Y-hat to softmax matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y = \n",
      "[[[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. ... 1. 0. 1.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 1. 0.]]\n",
      "\n",
      " [[0. 0. 0. ... 0. 0. 0.]]]\n",
      "trainY-shape = (5, 1, 2213)\n",
      "testY-shape = (5, 1, 150)\n"
     ]
    }
   ],
   "source": [
    "trainY = one_hot_matrix(trainY, C = 5)\n",
    "testY = one_hot_matrix(testY, C = 5) \n",
    "print(\"Y = \")\n",
    "print(trainY)\n",
    "print(\"trainY-shape = \" + str(trainY.shape))\n",
    "print(\"testY-shape = \" + str(testY.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\", \"W4\", \"b4\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z4 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    W4 = parameters['W4']\n",
    "    b4 = parameters['b4']\n",
    "    W5 = parameters['W5']\n",
    "    b5 = parameters['b5'] \n",
    "    W6 = parameters['W6']\n",
    "    b6 = parameters['b6'] \n",
    "    W7 = parameters['W7']\n",
    "    b7 = parameters['b7'] \n",
    "                                                                     \n",
    "    Z1 = tf.add(tf.matmul(W1,X) , b1 )                              \n",
    "    A1 = tf.nn.sigmoid(Z1)                                                  \n",
    "    Z2 = tf.add(tf.matmul(W2,A1), b2 )                               \n",
    "    A2 = tf.nn.dropout(Z2, keep_prob = 0.85)                                                  \n",
    "    Z3 = tf.add(tf.matmul(W3,A2), b3)                                \n",
    "    A3 = tf.nn.relu(Z3)                                         \n",
    "    Z4 = tf.add(tf.matmul(W4,A3), b4)                                \n",
    "    A4 = tf.nn.relu(Z4)                                                  \n",
    "    Z5 = tf.add(tf.matmul(W5,A4), b5)                                 \n",
    "    A5 = tf.nn.dropout(Z5, keep_prob=0.95)                                                  \n",
    "    Z6 = tf.add(tf.matmul(W6,A5), b6)                                 \n",
    "    A6 = tf.nn.relu(Z6)                                              \n",
    "    Z7 = tf.add(tf.matmul(W7,A6), b7)                                \n",
    "    return Z7\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z4 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z4\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(Z)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    #compute cost\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = Tensor(\"Mean:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    X, Y = create_placeholders(4, 1)\n",
    "    parameters = initialize_parameters(4,1)\n",
    "    Z1 = forward_propagation(X, parameters)\n",
    "    cost = compute_cost(Z1, Y)\n",
    "    print(\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "\n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    #permutation = list(np.random.permutation(m))\n",
    "    #shuffled_X = X[:, permutation]\n",
    "    #shuffled_Y = Y[:, 1, permutation]\n",
    "    \n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = X[:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "        mini_batch_Y = Y[:,:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = X[:, num_complete_minibatches*mini_batch_size : m]\n",
    "        mini_batch_Y = Y[:,:, num_complete_minibatches*mini_batch_size : m]\n",
    "        ### END CODE HERE ###\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the 1st mini_batch_X: (4, 64)\n",
      "shape of the 2nd mini_batch_X: (4, 64)\n",
      "shape of the 3rd mini_batch_X: (4, 64)\n",
      "shape of the last mini_batch_X: (4, 37)\n",
      "shape of the 1st mini_batch_Y: (5, 1, 37)\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1.\n",
      "   1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "   0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "mini_batches = random_mini_batches(trainX, trainY, 64, seed=0)\n",
    "\n",
    "print (\"shape of the 1st mini_batch_X: \" + str(mini_batches[0][0].shape))\n",
    "print (\"shape of the 2nd mini_batch_X: \" + str(mini_batches[1][0].shape))\n",
    "print (\"shape of the 3rd mini_batch_X: \" + str(mini_batches[2][0].shape))\n",
    "print (\"shape of the last mini_batch_X: \" + str(mini_batches[-1][0].shape))\n",
    "print (\"shape of the 1st mini_batch_Y: \" + str(mini_batches[-1][1].shape))\n",
    "print(mini_batches[1][1])\n",
    "#print (\"shape of the 2nd mini_batch_Y: \" + str(mini_batches[1].shape)) \n",
    "#print (\"shape of the 3rd mini_batch_Y: \" + str(mini_batches[2].shape))\n",
    "#print (\"shape of the last mini_batch_Y: \" + str(mini_batches[-1].shape))\n",
    "#print (\"mini batch sanity check: \")\n",
    "#mini_batches[0][1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(trainX, trainY, testX, testY, learning_rate = 0.003,\n",
    "          num_epochs = 4000, minibatch_size = 64, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = trainX.shape                                # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = trainY.shape[0]                                  # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    # Create Placeholders of shape (n_x, n_y)\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_x, n_h)\n",
    "\n",
    "    # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    Z = forward_propagation(X, parameters)\n",
    "    \n",
    "    \n",
    "    # Cost function: Add cost function to tensorflow graph\n",
    "    cost = compute_cost(Z,Y)\n",
    "\n",
    "    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "\n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "\n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(trainX, trainY, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "\n",
    "                epoch_cost += minibatch_cost / num_minibatches\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "\n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(Z), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: trainX, Y: trainY}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: testX, Y: testY}))\n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 1.654917\n",
      "Cost after epoch 100: 0.326112\n",
      "Cost after epoch 200: 0.323092\n",
      "Cost after epoch 300: 0.311590\n",
      "Cost after epoch 400: 0.303608\n",
      "Cost after epoch 500: 0.303037\n",
      "Cost after epoch 600: 0.302518\n",
      "Cost after epoch 700: 0.299770\n",
      "Cost after epoch 800: 0.298240\n",
      "Cost after epoch 900: 0.288743\n",
      "Cost after epoch 1000: 0.289696\n",
      "Cost after epoch 1100: 0.296370\n",
      "Cost after epoch 1200: 0.285127\n",
      "Cost after epoch 1300: 0.283468\n",
      "Cost after epoch 1400: 0.288080\n",
      "Cost after epoch 1500: 0.286286\n",
      "Cost after epoch 1600: 0.281456\n",
      "Cost after epoch 1700: 0.274733\n",
      "Cost after epoch 1800: 0.272805\n",
      "Cost after epoch 1900: 0.275317\n",
      "Cost after epoch 2000: 0.277789\n",
      "Cost after epoch 2100: 0.263436\n",
      "Cost after epoch 2200: 0.273782\n",
      "Cost after epoch 2300: 0.270973\n",
      "Cost after epoch 2400: 0.270555\n",
      "Cost after epoch 2500: 0.269557\n",
      "Cost after epoch 2600: 0.265733\n",
      "Cost after epoch 2700: 0.278372\n",
      "Cost after epoch 2800: 0.268932\n",
      "Cost after epoch 2900: 0.269485\n",
      "Cost after epoch 3000: 0.275955\n",
      "Cost after epoch 3100: 0.270986\n",
      "Cost after epoch 3200: 0.269001\n",
      "Cost after epoch 3300: 0.266666\n",
      "Cost after epoch 3400: 0.273026\n",
      "Cost after epoch 3500: 0.274347\n",
      "Cost after epoch 3600: 0.270475\n",
      "Cost after epoch 3700: 0.259380\n",
      "Cost after epoch 3800: 0.261963\n",
      "Cost after epoch 3900: 0.284611\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained!\n",
      "Train Accuracy: 0.9200181\n",
      "Test Accuracy: 0.9266667\n"
     ]
    }
   ],
   "source": [
    "# run the model \n",
    "parameters = model(trainX, trainY, testX,testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
